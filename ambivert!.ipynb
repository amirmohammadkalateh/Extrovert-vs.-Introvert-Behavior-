{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOv8SgB9PAwF+e4aKzLYKwK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amirmohammadkalateh/Extrovert-vs.-Introvert-Behavior-/blob/main/ambivert!.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cEZRfJa9fSvf",
        "outputId": "4d125e77-292e-4402-eaee-44e5a6bdbb83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Loading Data and Initial Inspection\n",
            "\n",
            "Error: 'content_fetcher' is not defined in this environment.\n",
            "Please ensure you are running this code in an environment where 'content_fetcher' is available (e.g., Google's collaborative Canvas).\n",
            "Attempting to load from a local file path as a fallback (this might fail if the file is not present locally)...\n",
            "Dataset loaded successfully from local file 'personality_datasert.csv'.\n",
            "\n",
            "First 5 rows of the dataset:\n",
            "   Time_spent_Alone Stage_fear  Social_event_attendance  Going_outside  \\\n",
            "0               4.0         No                      4.0            6.0   \n",
            "1               9.0        Yes                      0.0            0.0   \n",
            "2               9.0        Yes                      1.0            2.0   \n",
            "3               0.0         No                      6.0            7.0   \n",
            "4               3.0         No                      9.0            4.0   \n",
            "\n",
            "  Drained_after_socializing  Friends_circle_size  Post_frequency Personality  \n",
            "0                        No                 13.0             5.0   Extrovert  \n",
            "1                       Yes                  0.0             3.0   Introvert  \n",
            "2                       Yes                  5.0             2.0   Introvert  \n",
            "3                        No                 14.0             8.0   Extrovert  \n",
            "4                        No                  8.0             5.0   Extrovert  \n",
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2900 entries, 0 to 2899\n",
            "Data columns (total 8 columns):\n",
            " #   Column                     Non-Null Count  Dtype  \n",
            "---  ------                     --------------  -----  \n",
            " 0   Time_spent_Alone           2900 non-null   float64\n",
            " 1   Stage_fear                 2900 non-null   object \n",
            " 2   Social_event_attendance    2900 non-null   float64\n",
            " 3   Going_outside              2900 non-null   float64\n",
            " 4   Drained_after_socializing  2900 non-null   object \n",
            " 5   Friends_circle_size        2900 non-null   float64\n",
            " 6   Post_frequency             2900 non-null   float64\n",
            " 7   Personality                2900 non-null   object \n",
            "dtypes: float64(5), object(3)\n",
            "memory usage: 181.4+ KB\n",
            "None\n",
            "\n",
            "Step 2: Identifying Target Variable and Applying Label Encoding\n",
            "\n",
            "Automatically identified target column: 'Personality'\n",
            "  - Encoded feature column: 'Stage_fear'\n",
            "  - Encoded feature column: 'Drained_after_socializing'\n",
            "  - Encoded target column: 'Personality'\n",
            "\n",
            "First 5 rows of the dataset after Label Encoding:\n",
            "   Time_spent_Alone  Stage_fear  Social_event_attendance  Going_outside  \\\n",
            "0               4.0           0                      4.0            6.0   \n",
            "1               9.0           1                      0.0            0.0   \n",
            "2               9.0           1                      1.0            2.0   \n",
            "3               0.0           0                      6.0            7.0   \n",
            "4               3.0           0                      9.0            4.0   \n",
            "\n",
            "   Drained_after_socializing  Friends_circle_size  Post_frequency  Personality  \n",
            "0                          0                 13.0             5.0            0  \n",
            "1                          1                  0.0             3.0            1  \n",
            "2                          1                  5.0             2.0            1  \n",
            "3                          0                 14.0             8.0            0  \n",
            "4                          0                  8.0             5.0            0  \n",
            "\n",
            "Step 3: Separating Features (X) and Target (y)\n",
            "\n",
            "Features (X) shape: (2900, 7)\n",
            "Target (y) shape: (2900,)\n",
            "\n",
            "Step 4: Splitting Data into Training and Testing Sets\n",
            "\n",
            "Training features (X_train) shape: (2320, 7)\n",
            "Testing features (X_test) shape: (580, 7)\n",
            "Training target (y_train) shape: (2320,)\n",
            "Testing target (y_test) shape: (580,)\n",
            "\n",
            "--- Step 5: Training and Evaluating Machine Learning Model (RandomForestClassifier) ---\n",
            "\n",
            "Training RandomForestClassifier...\n",
            "RandomForestClassifier training complete.\n",
            "\n",
            "RandomForestClassifier Accuracy: 0.9000\n",
            "\n",
            "RandomForestClassifier Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Extrovert       0.91      0.89      0.90       298\n",
            "   Introvert       0.89      0.91      0.90       282\n",
            "\n",
            "    accuracy                           0.90       580\n",
            "   macro avg       0.90      0.90      0.90       580\n",
            "weighted avg       0.90      0.90      0.90       580\n",
            "\n",
            "\n",
            "--- Step 6: Training and Evaluating Deep Learning Model (Keras Sequential API) ---\n",
            "\n",
            "Features scaled using StandardScaler for Deep Learning model.\n",
            "Deep Learning model architecture created:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m65\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,345\u001b[0m (36.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,345</span> (36.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,345\u001b[0m (36.50 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,345</span> (36.50 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Compiling Deep Learning model...\n",
            "Deep Learning model compiled.\n",
            "\n",
            "Training Deep Learning model (this may take a moment)...\n",
            "Deep Learning model training complete.\n",
            "\n",
            "Evaluating Deep Learning model on test set...\n",
            "Deep Learning Model Loss: 0.2668\n",
            "Deep Learning Model Accuracy: 0.9172\n",
            "\n",
            "Deep Learning Model Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Extrovert       0.94      0.89      0.92       298\n",
            "   Introvert       0.89      0.94      0.92       282\n",
            "\n",
            "    accuracy                           0.92       580\n",
            "   macro avg       0.92      0.92      0.92       580\n",
            "weighted avg       0.92      0.92      0.92       580\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "import numpy as np # Import numpy for argmax in DL classification report\n",
        "\n",
        "# --- Step 1: Data Loading and Initial Inspection ---\n",
        "print(\"Step 1: Loading Data and Initial Inspection\\n\")\n",
        "\n",
        "# Use content_fetcher to load the dataset from the uploaded file\n",
        "try:\n",
        "    # Check if content_fetcher is defined in the current environment\n",
        "    if 'content_fetcher' in globals():\n",
        "        file_content = content_fetcher.fetch(\n",
        "            query=\"personality_datasert.csv\",\n",
        "            source_references=[{\"id\": \"uploaded:personality_datasert.csv\", \"type\": \"text/csv\"}]\n",
        "        )\n",
        "        df = pd.read_csv(io.StringIO(file_content))\n",
        "        print(\"Dataset loaded successfully from 'personality_datasert.csv' using content_fetcher.\")\n",
        "    else:\n",
        "        print(\"Error: 'content_fetcher' is not defined in this environment.\")\n",
        "        print(\"Please ensure you are running this code in an environment where 'content_fetcher' is available (e.g., Google's collaborative Canvas).\")\n",
        "        print(\"Attempting to load from a local file path as a fallback (this might fail if the file is not present locally)...\")\n",
        "        # Fallback for local execution if content_fetcher is not available\n",
        "        try:\n",
        "            df = pd.read_csv('personality_datasert.csv')\n",
        "            print(\"Dataset loaded successfully from local file 'personality_datasert.csv'.\")\n",
        "        except FileNotFoundError:\n",
        "            print(\"Error: 'personality_datasert.csv' not found locally either.\")\n",
        "            print(\"Please ensure the CSV file is in the same directory as the script or is accessible via the environment's file fetching mechanism.\")\n",
        "            exit() # Exit if data cannot be loaded\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during data loading: {e}\")\n",
        "    # Exit if data loading fails, as subsequent steps depend on it.\n",
        "    exit()\n",
        "\n",
        "# Display the first few rows of the dataframe\n",
        "print(\"\\nFirst 5 rows of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "# Display concise summary of the dataframe, including data types\n",
        "print(\"\\nDataset Info:\")\n",
        "print(df.info())\n",
        "\n",
        "# --- Step 2: Identify Target Variable and Apply Label Encoding ---\n",
        "print(\"\\nStep 2: Identifying Target Variable and Applying Label Encoding\\n\")\n",
        "\n",
        "# Automatically determine the target column.\n",
        "# We prioritize 'Personality (English)' or 'Personality' as it's a personality dataset.\n",
        "target_column = None\n",
        "if 'Personality (English)' in df.columns:\n",
        "    target_column = 'Personality (English)'\n",
        "elif 'Personality' in df.columns:\n",
        "    target_column = 'Personality'\n",
        "elif 'type' in df.columns: # Common for MBTI types\n",
        "    target_column = 'type'\n",
        "else:\n",
        "    # Fallback: If no clear personality-related column, try to find the last object column\n",
        "    # or the one with the most unique categorical values as a potential target.\n",
        "    object_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "    if object_cols:\n",
        "        # Prioritize a column that has relatively few unique values for classification\n",
        "        best_candidate = None\n",
        "        min_unique = float('inf')\n",
        "        for col in object_cols:\n",
        "            n_unique = df[col].nunique()\n",
        "            if n_unique > 1 and n_unique < min_unique: # Must have more than 1 unique value\n",
        "                min_unique = n_unique\n",
        "                best_candidate = col\n",
        "        if best_candidate:\n",
        "            target_column = best_candidate\n",
        "        else:\n",
        "            print(\"No suitable categorical columns found for a classification target. Please specify a target column manually if it's not detected.\")\n",
        "            exit() # Cannot proceed without a target\n",
        "    else:\n",
        "        print(\"No categorical columns found for a classification target. Please specify a target column.\")\n",
        "        exit() # Cannot proceed without a target\n",
        "\n",
        "print(f\"Automatically identified target column: '{target_column}'\")\n",
        "\n",
        "# Identify all categorical columns for encoding (including potential features and the target)\n",
        "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# Initialize a dictionary to store LabelEncoders for inverse transformation later\n",
        "label_encoders = {}\n",
        "\n",
        "# Apply Label Encoding to all categorical feature columns\n",
        "for col in categorical_cols:\n",
        "    if col != target_column: # Exclude the target column for now, it's encoded separately\n",
        "        le = LabelEncoder()\n",
        "        df[col] = le.fit_transform(df[col])\n",
        "        label_encoders[col] = le # Store the encoder\n",
        "        print(f\"  - Encoded feature column: '{col}'\")\n",
        "\n",
        "# Apply Label Encoding to the target column\n",
        "le_target = LabelEncoder()\n",
        "df[target_column] = le_target.fit_transform(df[target_column])\n",
        "label_encoders[target_column] = le_target # Store the target encoder\n",
        "print(f\"  - Encoded target column: '{target_column}'\")\n",
        "\n",
        "# Display the dataframe after encoding to show numerical conversion\n",
        "print(\"\\nFirst 5 rows of the dataset after Label Encoding:\")\n",
        "print(df.head())\n",
        "\n",
        "# --- Step 3: Separate Features (X) and Target (y) ---\n",
        "print(\"\\nStep 3: Separating Features (X) and Target (y)\\n\")\n",
        "\n",
        "X = df.drop(columns=[target_column]) # Features are all columns except the target\n",
        "y = df[target_column]              # Target variable\n",
        "\n",
        "print(f\"Features (X) shape: {X.shape}\")\n",
        "print(f\"Target (y) shape: {y.shape}\")\n",
        "\n",
        "# --- Step 4: Split Data into Training and Testing Sets ---\n",
        "print(\"\\nStep 4: Splitting Data into Training and Testing Sets\\n\")\n",
        "\n",
        "# Split the dataset into training (80%) and testing (20%) sets\n",
        "# stratify=y ensures that the proportion of classes in y is the same in both train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training features (X_train) shape: {X_train.shape}\")\n",
        "print(f\"Testing features (X_test) shape: {X_test.shape}\")\n",
        "print(f\"Training target (y_train) shape: {y_train.shape}\")\n",
        "print(f\"Testing target (y_test) shape: {y_test.shape}\")\n",
        "\n",
        "# --- Step 5: Machine Learning Model (RandomForestClassifier) ---\n",
        "print(\"\\n--- Step 5: Training and Evaluating Machine Learning Model (RandomForestClassifier) ---\\n\")\n",
        "\n",
        "# Initialize and train the RandomForestClassifier\n",
        "ml_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "print(\"Training RandomForestClassifier...\")\n",
        "ml_model.fit(X_train, y_train)\n",
        "print(\"RandomForestClassifier training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_ml = ml_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_ml = accuracy_score(y_test, y_pred_ml)\n",
        "print(f\"\\nRandomForestClassifier Accuracy: {accuracy_ml:.4f}\")\n",
        "\n",
        "print(\"\\nRandomForestClassifier Classification Report:\")\n",
        "# Get the original class names for a more readable classification report\n",
        "target_names_ml = le_target.inverse_transform(range(len(le_target.classes_)))\n",
        "print(classification_report(y_test, y_pred_ml, target_names=target_names_ml, zero_division=0))\n",
        "\n",
        "# --- Step 6: Deep Learning Model (Keras Sequential API) ---\n",
        "print(\"\\n--- Step 6: Training and Evaluating Deep Learning Model (Keras Sequential API) ---\\n\")\n",
        "\n",
        "# Preprocessing for Deep Learning: Scale numerical features\n",
        "# Neural networks often perform better with scaled input data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "print(\"Features scaled using StandardScaler for Deep Learning model.\")\n",
        "\n",
        "# Determine the number of output units and activation based on the number of classes\n",
        "num_classes = len(le_target.classes_)\n",
        "if num_classes == 2:\n",
        "    # Binary classification\n",
        "    dl_output_units = 1\n",
        "    dl_activation = 'sigmoid'\n",
        "    dl_loss = 'binary_crossentropy'\n",
        "    y_train_dl = y_train.astype('float32')\n",
        "    y_test_dl = y_test.astype('float32')\n",
        "else:\n",
        "    # Multi-class classification\n",
        "    dl_output_units = num_classes\n",
        "    dl_activation = 'softmax'\n",
        "    dl_loss = 'sparse_categorical_crossentropy' # Use this when labels are integer encoded\n",
        "    y_train_dl = y_train.astype('int32')\n",
        "    y_test_dl = y_test.astype('int32')\n",
        "\n",
        "# Build the Sequential Deep Learning Model\n",
        "dl_model = Sequential([\n",
        "    # Input layer and first hidden layer\n",
        "    Dense(units=128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
        "    # Second hidden layer\n",
        "    Dense(units=64, activation='relu'),\n",
        "    # Output layer\n",
        "    Dense(units=dl_output_units, activation=dl_activation)\n",
        "])\n",
        "print(\"Deep Learning model architecture created:\")\n",
        "dl_model.summary()\n",
        "\n",
        "# Compile the Deep Learning model\n",
        "print(\"\\nCompiling Deep Learning model...\")\n",
        "dl_model.compile(optimizer='adam',\n",
        "                 loss=dl_loss,\n",
        "                 metrics=['accuracy'])\n",
        "print(\"Deep Learning model compiled.\")\n",
        "\n",
        "# Train the Deep Learning model\n",
        "print(\"\\nTraining Deep Learning model (this may take a moment)...\")\n",
        "try:\n",
        "    history = dl_model.fit(X_train_scaled, y_train_dl,\n",
        "                           epochs=100,      # Number of training iterations\n",
        "                           batch_size=32,   # Number of samples per gradient update\n",
        "                           validation_split=0.1, # Use 10% of training data for validation\n",
        "                           verbose=0)       # Set to 1 for progress bar, 0 for silent\n",
        "    print(\"Deep Learning model training complete.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error during Deep Learning model training: {e}\")\n",
        "    print(\"Training might have failed or partially completed. Proceeding to evaluation.\")\n",
        "    pass\n",
        "\n",
        "\n",
        "# Evaluate the Deep Learning model on the test set\n",
        "print(\"\\nEvaluating Deep Learning model on test set...\")\n",
        "loss_dl, accuracy_dl = dl_model.evaluate(X_test_scaled, y_test_dl, verbose=0)\n",
        "print(f\"Deep Learning Model Loss: {loss_dl:.4f}\")\n",
        "print(f\"Deep Learning Model Accuracy: {accuracy_dl:.4f}\")\n",
        "\n",
        "print(\"\\nDeep Learning Model Classification Report:\")\n",
        "# Predict probabilities for classification report\n",
        "y_pred_proba_dl = dl_model.predict(X_test_scaled, verbose=0)\n",
        "\n",
        "# Convert probabilities to class labels\n",
        "if num_classes == 2:\n",
        "    y_pred_dl_classes = (y_pred_proba_dl > 0.5).astype(int).flatten()\n",
        "else:\n",
        "    y_pred_dl_classes = np.argmax(y_pred_proba_dl, axis=1)\n",
        "\n",
        "target_names_dl = le_target.inverse_transform(range(len(le_target.classes_)))\n",
        "print(classification_report(y_test, y_pred_dl_classes, target_names=target_names_dl, zero_division=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Yu6IBYK_j1nY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikeras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kOq6xyOj9FK",
        "outputId": "7d273343-3542-4ce0-b716-954559410930"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Requirement already satisfied: scikit-learn>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from scikeras) (1.6.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.16.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.14.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade scikit-learn tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x_8KEhMIkPST",
        "outputId": "97d30fcb-c890-49ee-c060-65ca58aa5cee"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: numpy>=1.22.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m885.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m99.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ml-dtypes, tensorboard, scikit-learn, tensorflow\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.4.1\n",
            "    Uninstalling ml-dtypes-0.4.1:\n",
            "      Successfully uninstalled ml-dtypes-0.4.1\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.18.0\n",
            "    Uninstalling tensorboard-2.18.0:\n",
            "      Successfully uninstalled tensorboard-2.18.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.18.0\n",
            "    Uninstalling tensorflow-2.18.0:\n",
            "      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ml-dtypes-0.5.1 scikit-learn-1.7.0 tensorboard-2.19.0 tensorflow-2.19.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn",
                  "tensorflow"
                ]
              },
              "id": "c8087e9cbd5d4abe862887c2d9b2bb36"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall existing packages to ensure a clean slate\n",
        "!pip uninstall -y scikeras scikit-learn tensorflow\n",
        "\n",
        "# Install scikeras, which will install a compatible version of scikit-learn and tensorflow\n",
        "!pip install scikeras\n",
        "\n",
        "# Optionally, if you need a specific version of tensorflow, install it afterwards.\n",
        "# Note: installing a version incompatible with scikeras might reintroduce the error.\n",
        "# !pip install tensorflow==<your_desired_version>\n",
        "\n",
        "# Verify the installed versions\n",
        "!pip show scikeras scikit-learn tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JRCWLpT2kUcX",
        "outputId": "30dbb7ef-9d3d-42f2-b522-eb3e3147d70b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: scikeras 0.13.0\n",
            "Uninstalling scikeras-0.13.0:\n",
            "  Successfully uninstalled scikeras-0.13.0\n",
            "Found existing installation: scikit-learn 1.7.0\n",
            "Uninstalling scikit-learn-1.7.0:\n",
            "  Successfully uninstalled scikit-learn-1.7.0\n",
            "Found existing installation: tensorflow 2.19.0\n",
            "Uninstalling tensorflow-2.19.0:\n",
            "  Successfully uninstalled tensorflow-2.19.0\n",
            "Collecting scikeras\n",
            "  Using cached scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Collecting scikit-learn>=1.4.2 (from scikeras)\n",
            "  Using cached scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (3.14.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.16.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (24.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.4.2->scikeras) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from optree->keras>=3.2.0->scikeras) (4.14.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Using cached scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Using cached scikit_learn-1.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
            "Installing collected packages: scikit-learn, scikeras\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scikeras-0.13.0 scikit-learn-1.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "scikeras",
                  "sklearn"
                ]
              },
              "id": "8e991cd2f64240deb62676901b1b61df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Package(s) not found: tensorflow\u001b[0m\u001b[33m\n",
            "\u001b[0mName: scikeras\n",
            "Version: 0.13.0\n",
            "Summary: Scikit-Learn API wrapper for Keras.\n",
            "Home-page: https://github.com/adriangb/scikeras\n",
            "Author: Adrian Garcia Badaracco\n",
            "Author-email: 1755071+adriangb@users.noreply.github.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: keras, scikit-learn\n",
            "Required-by: \n",
            "---\n",
            "Name: scikit-learn\n",
            "Version: 1.7.0\n",
            "Summary: A set of python modules for machine learning and data mining\n",
            "Home-page: https://scikit-learn.org\n",
            "Author: \n",
            "Author-email: \n",
            "License: BSD 3-Clause License\n",
            "\n",
            " Copyright (c) 2007-2024 The scikit-learn developers.\n",
            " All rights reserved.\n",
            "\n",
            " Redistribution and use in source and binary forms, with or without\n",
            " modification, are permitted provided that the following conditions are met:\n",
            "\n",
            " * Redistributions of source code must retain the above copyright notice, this\n",
            "   list of conditions and the following disclaimer.\n",
            "\n",
            " * Redistributions in binary form must reproduce the above copyright notice,\n",
            "   this list of conditions and the following disclaimer in the documentation\n",
            "   and/or other materials provided with the distribution.\n",
            "\n",
            " * Neither the name of the copyright holder nor the names of its\n",
            "   contributors may be used to endorse or promote products derived from\n",
            "   this software without specific prior written permission.\n",
            "\n",
            " THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
            " AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
            " IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n",
            " DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n",
            " FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n",
            " DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n",
            " SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n",
            " CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n",
            " OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n",
            " OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n",
            "\n",
            " ----\n",
            "\n",
            " This binary distribution of scikit-learn also bundles the following software:\n",
            "\n",
            " ----\n",
            "\n",
            " Name: GCC runtime library\n",
            " Files: scikit_learn.libs/libgomp*.so*\n",
            " Availability: https://gcc.gnu.org/git/?p=gcc.git;a=tree;f=libgomp\n",
            "\n",
            " GCC RUNTIME LIBRARY EXCEPTION\n",
            "\n",
            " Version 3.1, 31 March 2009\n",
            "\n",
            " Copyright (C) 2009 Free Software Foundation, Inc. <http://fsf.org/>\n",
            "\n",
            " Everyone is permitted to copy and distribute verbatim copies of this\n",
            " license document, but changing it is not allowed.\n",
            "\n",
            " This GCC Runtime Library Exception (\"Exception\") is an additional\n",
            " permission under section 7 of the GNU General Public License, version\n",
            " 3 (\"GPLv3\"). It applies to a given file (the \"Runtime Library\") that\n",
            " bears a notice placed by the copyright holder of the file stating that\n",
            " the file is governed by GPLv3 along with this Exception.\n",
            "\n",
            " When you use GCC to compile a program, GCC may combine portions of\n",
            " certain GCC header files and runtime libraries with the compiled\n",
            " program. The purpose of this Exception is to allow compilation of\n",
            " non-GPL (including proprietary) programs to use, in this way, the\n",
            " header files and runtime libraries covered by this Exception.\n",
            "\n",
            " 0. Definitions.\n",
            "\n",
            " A file is an \"Independent Module\" if it either requires the Runtime\n",
            " Library for execution after a Compilation Process, or makes use of an\n",
            " interface provided by the Runtime Library, but is not otherwise based\n",
            " on the Runtime Library.\n",
            "\n",
            " \"GCC\" means a version of the GNU Compiler Collection, with or without\n",
            " modifications, governed by version 3 (or a specified later version) of\n",
            " the GNU General Public License (GPL) with the option of using any\n",
            " subsequent versions published by the FSF.\n",
            "\n",
            " \"GPL-compatible Software\" is software whose conditions of propagation,\n",
            " modification and use would permit combination with GCC in accord with\n",
            " the license of GCC.\n",
            "\n",
            " \"Target Code\" refers to output from any compiler for a real or virtual\n",
            " target processor architecture, in executable form or suitable for\n",
            " input to an assembler, loader, linker and/or execution\n",
            " phase. Notwithstanding that, Target Code does not include data in any\n",
            " format that is used as a compiler intermediate representation, or used\n",
            " for producing a compiler intermediate representation.\n",
            "\n",
            " The \"Compilation Process\" transforms code entirely represented in\n",
            " non-intermediate languages designed for human-written code, and/or in\n",
            " Java Virtual Machine byte code, into Target Code. Thus, for example,\n",
            " use of source code generators and preprocessors need not be considered\n",
            " part of the Compilation Process, since the Compilation Process can be\n",
            " understood as starting with the output of the generators or\n",
            " preprocessors.\n",
            "\n",
            " A Compilation Process is \"Eligible\" if it is done using GCC, alone or\n",
            " with other GPL-compatible software, or if it is done without using any\n",
            " work based on GCC. For example, using non-GPL-compatible Software to\n",
            " optimize any GCC intermediate representations would not qualify as an\n",
            " Eligible Compilation Process.\n",
            "\n",
            " 1. Grant of Additional Permission.\n",
            "\n",
            " You have permission to propagate a work of Target Code formed by\n",
            " combining the Runtime Library with Independent Modules, even if such\n",
            " propagation would otherwise violate the terms of GPLv3, provided that\n",
            " all Target Code was generated by Eligible Compilation Processes. You\n",
            " may then convey such a combination under terms of your choice,\n",
            " consistent with the licensing of the Independent Modules.\n",
            "\n",
            " 2. No Weakening of GCC Copyleft.\n",
            "\n",
            " The availability of this Exception does not imply any general\n",
            " presumption that third-party software is unaffected by the copyleft\n",
            " requirements of the license of GCC.\n",
            "\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: joblib, numpy, scipy, threadpoolctl\n",
            "Required-by: fastai, hdbscan, imbalanced-learn, libpysal, librosa, mlxtend, pynndescent, scikeras, sentence-transformers, shap, sklearn-compat, sklearn-pandas, tsfresh, umap-learn, yellowbrick\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages if not already present\n",
        "!pip install scikeras tensorflow scikit-learn\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.preprocessing import StandardScaler # Ensure StandardScaler is imported\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "print(\"Step 1: Loading the dataset...\")\n",
        "try:\n",
        "    # The original code included content_fetcher logic, which is specific to certain environments.\n",
        "    # For a general Jupyter notebook, loading from a local file is standard.\n",
        "    # Assuming 'personality_datasert.csv' is in the same directory.\n",
        "    df = pd.read_csv('personality_datasert.csv')\n",
        "    print(\"Dataset loaded successfully.\")\n",
        "    print(\"Initial 5 rows of the dataset:\")\n",
        "    display(df.head()) # Using display instead of print for better notebook formatting\n",
        "    print(\"\\nDataset Info:\")\n",
        "    df.info()\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'personality_datasert.csv' not found. Please ensure the file is in the correct directory.\")\n",
        "    # Use a more robust way to handle script termination in notebooks if needed,\n",
        "    # but for simple cases, exiting the cell is often sufficient.\n",
        "    # raise # Re-raise the exception to stop execution if the file is missing.\n",
        "    # For demonstration, we'll just print the error and allow the user to fix.\n",
        "    print(\"Please upload the 'personality_datasert.csv' file or place it in the correct path.\")\n",
        "\n",
        "\n",
        "# Check if df was loaded successfully before proceeding\n",
        "if 'df' not in locals():\n",
        "    print(\"Dataframe not loaded. Exiting script.\")\n",
        "    # To strictly stop execution here, you might use:\n",
        "    # get_ipython().run_cell_magic('javascript', '', 'IPython.notebook.execute_cells_after(this.cell_idx+1)')\n",
        "    # get_ipython().run_cell_magic('python', '', 'raise SystemExit')\n",
        "    # but letting the user see the error and fix the file path is usually better.\n",
        "    # For this fix, we assume the user will ensure the file is present.\n",
        "    # If the file isn't loaded, the subsequent steps will likely fail anyway.\n",
        "else:\n",
        "    # Step 2: Data Preprocessing\n",
        "    print(\"\\nStep 2: Data Preprocessing...\")\n",
        "\n",
        "    # Handle mixed types or non-numeric values by coercing to numeric, then fill NaN\n",
        "    # Convert relevant columns to numeric, coercing errors will turn non-numeric into NaN\n",
        "    numeric_cols_to_process = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency']\n",
        "    for col in numeric_cols_to_process:\n",
        "        if col in df.columns:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "            # Fill NaN values that might have resulted from 'coerce' with the median\n",
        "            if df[col].isnull().any():\n",
        "                median_val = df[col].median()\n",
        "                df[col] = df[col].fillna(median_val)\n",
        "                print(f\"Filled NaN in '{col}' with median value: {median_val}\")\n",
        "        else:\n",
        "            print(f\"Warning: Column '{col}' not found in DataFrame.\")\n",
        "\n",
        "\n",
        "    # Identify categorical columns (excluding the target 'Personality')\n",
        "    # Check if the target column exists before proceeding\n",
        "    target_column = 'Personality'\n",
        "    if target_column not in df.columns:\n",
        "        print(f\"Error: Target column '{target_column}' not found in the dataset.\")\n",
        "        print(\"Please check the column name in your CSV file.\")\n",
        "        # Similar to file not found, you'd ideally stop execution if the target is missing.\n",
        "    else:\n",
        "        # Identify categorical features dynamically, excluding the target\n",
        "        categorical_features = df.select_dtypes(include='object').columns.tolist()\n",
        "        if target_column in categorical_features:\n",
        "             categorical_features.remove(target_column)\n",
        "\n",
        "        print(f\"Categorical features identified for encoding: {categorical_features}\")\n",
        "\n",
        "        # Initialize OrdinalEncoder for features and LabelEncoder for the target\n",
        "        encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "\n",
        "        # Apply OrdinalEncoder to categorical features if any exist\n",
        "        if categorical_features:\n",
        "            # Apply fit_transform only to the identified categorical features\n",
        "            df[categorical_features] = encoder.fit_transform(df[categorical_features])\n",
        "            print(\"Categorical features encoded using OrdinalEncoder.\")\n",
        "        else:\n",
        "             print(\"No categorical features found for encoding (excluding the target).\")\n",
        "\n",
        "\n",
        "        # Separate features (X) and target (y)\n",
        "        X = df.drop(target_column, axis=1)\n",
        "        y = df[target_column]\n",
        "\n",
        "        # Encode the target variable 'Personality' using LabelEncoder\n",
        "        label_encoder = LabelEncoder()\n",
        "        y_encoded = label_encoder.fit_transform(y)\n",
        "        print(\"Target variable 'Personality' encoded using LabelEncoder.\")\n",
        "        print(f\"Original classes: {label_encoder.classes_}\")\n",
        "        print(f\"Encoded labels: {np.unique(y_encoded)}\")\n",
        "\n",
        "        # Check if the target variable is binary (2 classes) for binary classification setup\n",
        "        num_classes = len(label_encoder.classes_)\n",
        "        if num_classes != 2:\n",
        "            print(f\"Warning: The target variable has {num_classes} classes. The current model setup is for binary classification.\")\n",
        "            print(\"Consider adjusting the model architecture and loss function for multi-class classification.\")\n",
        "            # For multi-class, output layer should have `num_classes` units with 'softmax' activation,\n",
        "            # and loss should be 'sparse_categorical_crossentropy'.\n",
        "\n",
        "        # Split the data into training and testing sets\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n",
        "        print(f\"Data split into training (X_train shape: {X_train.shape}, y_train shape: {y_train.shape})\")\n",
        "        print(f\"and testing (X_test shape: {X_test.shape}, y_test shape: {y_test.shape}) sets.\")\n",
        "\n",
        "        # Feature Scaling (StandardScaler)\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        print(\"Features scaled using StandardScaler.\")\n",
        "\n",
        "        # Step 3: Define the Deep Learning Model\n",
        "        print(\"\\nStep 3: Defining the Deep Learning Model...\")\n",
        "\n",
        "        # Function to create Keras model for use with KerasClassifier\n",
        "        def create_model(optimizer='adam', learning_rate=0.001, activation='relu', neurons=(64, 32)):\n",
        "            # Set learning rate for the optimizer\n",
        "            # Check for optimizer existence in tf.keras.optimizers\n",
        "            optimizers = {\n",
        "                'adam': tf.keras.optimizers.Adam,\n",
        "                'rmsprop': tf.keras.optimizers.RMSprop,\n",
        "                'sgd': tf.keras.optimizers.SGD # Add SGD option for completeness\n",
        "            }\n",
        "            opt_class = optimizers.get(optimizer.lower(), tf.keras.optimizers.Adam) # Default to Adam\n",
        "            opt = opt_class(learning_rate=learning_rate)\n",
        "\n",
        "\n",
        "            model = Sequential()\n",
        "            # Input layer and first hidden layer\n",
        "            # Ensure input_shape is correct based on scaled features\n",
        "            model.add(Dense(neurons[0], input_shape=(X_train_scaled.shape[1],), activation=activation))\n",
        "            # Additional hidden layers\n",
        "            for i in range(1, len(neurons)):\n",
        "                model.add(Dense(neurons[i], activation=activation))\n",
        "\n",
        "            # Output layer: 2 classes (Extrovert, Introvert) -> sigmoid for binary classification\n",
        "            # If you need multi-class, change units to num_classes and activation to 'softmax'\n",
        "            if num_classes == 2:\n",
        "                model.add(Dense(1, activation='sigmoid'))\n",
        "                loss_func = 'binary_crossentropy'\n",
        "            else:\n",
        "                model.add(Dense(num_classes, activation='softmax'))\n",
        "                loss_func = 'sparse_categorical_crossentropy' # Use this for integer labels\n",
        "\n",
        "\n",
        "            model.compile(optimizer=opt, loss=loss_func, metrics=['accuracy'])\n",
        "            return model\n",
        "\n",
        "        # Step 4: Hyperparameter Tuning using GridSearchCV\n",
        "        print(\"\\nStep 4: Performing Hyperparameter Tuning using GridSearchCV...\")\n",
        "\n",
        "        # Create KerasClassifier with a default model\n",
        "        # Pass num_classes to the wrapper if needed by create_model (currently not explicitly used, but good practice)\n",
        "        keras_model = KerasClassifier(model=create_model, verbose=0, # Set verbose=0 for silent training during grid search\n",
        "                                      loss='binary_crossentropy', # Specify loss directly if always binary\n",
        "                                      metrics=['accuracy'])\n",
        "\n",
        "        # Define the parameter grid for GridSearchCV\n",
        "        param_grid = {\n",
        "            'model__optimizer': ['adam', 'rmsprop'],\n",
        "            'model__learning_rate': [0.001, 0.01],\n",
        "            'model__activation': ['relu', 'tanh'],\n",
        "            'model__neurons': [(32, 16), (64, 32), (128, 64, 32)], # Different network architectures\n",
        "            'batch_size': [16, 32],\n",
        "            'epochs': [50, 100] # Set max epochs, early stopping will manage the actual number\n",
        "        }\n",
        "\n",
        "        # Step 5: Use EarlyStopping callback\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "        # Initialize GridSearchCV\n",
        "        grid_search = GridSearchCV(estimator=keras_model,\n",
        "                                   param_grid=param_grid,\n",
        "                                   scoring='accuracy',\n",
        "                                   cv=3,\n",
        "                                   verbose=1,\n",
        "                                   n_jobs=-1,\n",
        "                                   error_score='raise'\n",
        "                                  )\n",
        "\n",
        "        # Fit GridSearchCV\n",
        "        print(\"Fitting GridSearchCV (this may take a while)...\")\n",
        "        try:\n",
        "            # Pass callbacks to the fit method of the KerasClassifier which GridSearchCV calls\n",
        "            grid_result = grid_search.fit(X_train_scaled, y_train,\n",
        "                                          callbacks=[early_stopping],\n",
        "                                          validation_split=0.1 # Use part of the training data for validation\n",
        "                                         )\n",
        "\n",
        "            print(\"\\nGridSearchCV completed.\")\n",
        "            print(f\"Best: {grid_result.best_score_:.4f} using {grid_result.best_params_}\")\n",
        "\n",
        "            # Step 6: Evaluate the best model\n",
        "            print(\"\\nStep 6: Evaluating the best model on the test set...\")\n",
        "\n",
        "            best_model = grid_result.best_estimator_\n",
        "            # predict_proba returns probabilities, predict returns class labels\n",
        "            # KerasClassifier predict_proba method returns predictions compatible with sklearn expectations\n",
        "            y_pred_proba = best_model.predict_proba(X_test_scaled)\n",
        "\n",
        "            # For binary classification, y_pred_proba will have shape (n_samples, 2) if output is Dense(2, 'softmax'),\n",
        "            # or (n_samples, 1) if output is Dense(1, 'sigmoid').\n",
        "            # If Dense(1, 'sigmoid'), y_pred_proba will be probabilities of class 1.\n",
        "            # If Dense(2, 'softmax'), y_pred_proba[:, 1] is probability of class 1.\n",
        "            # We need to handle this based on the model's output layer.\n",
        "            # The create_model currently uses Dense(1, 'sigmoid') for binary.\n",
        "            # So y_pred_proba will be shape (n_samples, 1).\n",
        "            # We need to convert these probabilities to class labels (0 or 1).\n",
        "            # The predict() method of KerasClassifier already handles this binary conversion\n",
        "            # for a Dense(1, 'sigmoid') output by default thresholding at 0.5.\n",
        "            y_pred = best_model.predict(X_test_scaled) # This will return class indices (0 or 1) directly\n",
        "\n",
        "            print(f\"\\nAccuracy on test set: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "            print(\"\\nClassification Report on test set:\")\n",
        "            # classification_report needs true labels (y_test) and predicted labels (y_pred)\n",
        "            print(classification_report(y_test, y_pred, target_names=label_encoder.classes_, zero_division=0))\n",
        "\n",
        "            print(\"\\nCode execution completed successfully.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn error occurred during GridSearchCV or evaluation: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc() # Print full traceback for debugging\n",
        "            print(\"Please review the error message and traceback to diagnose the issue.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0qYebsbQguvY",
        "outputId": "8a67f507-7794-4426-a8cd-905a6a505105"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikeras in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from scikeras) (3.8.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.2.0->scikeras) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.2.0->scikeras) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->scikeras) (0.1.2)\n",
            "Using cached tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "Installing collected packages: tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tensorflow-2.19.0\n",
            "Step 1: Loading the dataset...\n",
            "Dataset loaded successfully.\n",
            "Initial 5 rows of the dataset:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   Time_spent_Alone Stage_fear  Social_event_attendance  Going_outside  \\\n",
              "0               4.0         No                      4.0            6.0   \n",
              "1               9.0        Yes                      0.0            0.0   \n",
              "2               9.0        Yes                      1.0            2.0   \n",
              "3               0.0         No                      6.0            7.0   \n",
              "4               3.0         No                      9.0            4.0   \n",
              "\n",
              "  Drained_after_socializing  Friends_circle_size  Post_frequency Personality  \n",
              "0                        No                 13.0             5.0   Extrovert  \n",
              "1                       Yes                  0.0             3.0   Introvert  \n",
              "2                       Yes                  5.0             2.0   Introvert  \n",
              "3                        No                 14.0             8.0   Extrovert  \n",
              "4                        No                  8.0             5.0   Extrovert  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-29a768ac-41de-4cdf-8dc6-ec38ec87b1e6\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time_spent_Alone</th>\n",
              "      <th>Stage_fear</th>\n",
              "      <th>Social_event_attendance</th>\n",
              "      <th>Going_outside</th>\n",
              "      <th>Drained_after_socializing</th>\n",
              "      <th>Friends_circle_size</th>\n",
              "      <th>Post_frequency</th>\n",
              "      <th>Personality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.0</td>\n",
              "      <td>No</td>\n",
              "      <td>4.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>No</td>\n",
              "      <td>13.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Extrovert</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>Introvert</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>5.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>Introvert</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>6.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>No</td>\n",
              "      <td>14.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>Extrovert</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3.0</td>\n",
              "      <td>No</td>\n",
              "      <td>9.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>No</td>\n",
              "      <td>8.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>Extrovert</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "      \n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-29a768ac-41de-4cdf-8dc6-ec38ec87b1e6')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "      \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-29a768ac-41de-4cdf-8dc6-ec38ec87b1e6 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-29a768ac-41de-4cdf-8dc6-ec38ec87b1e6');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "  \n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2900 entries, 0 to 2899\n",
            "Data columns (total 8 columns):\n",
            " #   Column                     Non-Null Count  Dtype  \n",
            "---  ------                     --------------  -----  \n",
            " 0   Time_spent_Alone           2900 non-null   float64\n",
            " 1   Stage_fear                 2900 non-null   object \n",
            " 2   Social_event_attendance    2900 non-null   float64\n",
            " 3   Going_outside              2900 non-null   float64\n",
            " 4   Drained_after_socializing  2900 non-null   object \n",
            " 5   Friends_circle_size        2900 non-null   float64\n",
            " 6   Post_frequency             2900 non-null   float64\n",
            " 7   Personality                2900 non-null   object \n",
            "dtypes: float64(5), object(3)\n",
            "memory usage: 181.4+ KB\n",
            "\n",
            "Step 2: Data Preprocessing...\n",
            "Categorical features identified for encoding: ['Stage_fear', 'Drained_after_socializing']\n",
            "Categorical features encoded using OrdinalEncoder.\n",
            "Target variable 'Personality' encoded using LabelEncoder.\n",
            "Original classes: ['Extrovert' 'Introvert']\n",
            "Encoded labels: [0 1]\n",
            "Data split into training (X_train shape: (2320, 7), y_train shape: (2320,))\n",
            "and testing (X_test shape: (580, 7), y_test shape: (580,)) sets.\n",
            "Features scaled using StandardScaler.\n",
            "\n",
            "Step 3: Defining the Deep Learning Model...\n",
            "\n",
            "Step 4: Performing Hyperparameter Tuning using GridSearchCV...\n",
            "Fitting GridSearchCV (this may take a while)...\n",
            "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
          ]
        }
      ]
    }
  ]
}